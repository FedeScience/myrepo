# Machine Learning Pipeline 
(Python https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

1) Import Data 
  1.1) web
  1.2) database
  1.3) file
  1.4) paper
  
2) Clean Data https://elitedatascience.com/data-cleaning 
  2.1)Remove Unwanted observations
    1.1.1)duplicate
    1.1.2)irrelevant
  2.2) Fix Structural Errors
    1.2.1)inconsistent capitalization
    1.2.2)mislabeled classes
    1.2.3)FIRST NAME+LAST NAME=CUSTOMER
  2.3) Remove Outliers https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/
    2.3.1) Standard Deviation https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/
    2.3.2) Z-Score https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba
    2.3.3) Interquartile Range
  2.4) Missing Data
    2.4.1)Missing categorical data
      2.4.1.1)label them as ’Missing’
    2.4.2)Missing numeric data ~ Imputing (mean,mode,median)
      2.4.2.1)Flag the observation with an indicator variable
      2.4.2.2)fill the original missing value with 0
  2.5) Transform Data https://machinelearningmastery.com/improve-model-accuracy-with-data-pre-processing/
    2.5.1)Centering
    2.5.2)Scaling https://medium.com/@TheDataGyan/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55
      2.5.2.1)Min-Max
      2.5.2.2)Z-score	
    2.5.3)Remove Skew
      2.5.3.1)log
      2.5.3.2)square root
      2.5.3.3)inverse
      2.5.3.4)Box-Cox
    2.5.4)Binning
      
3) Feature Engineering https://elitedatascience.com/feature-engineering
  3.1)Infuse Domain Knowledge
    3.1.2)indicator binary variable (conditions are met?)
  3.2)Create Interaction Features
    3.2.1)products
    3.2.2)sums
    3.2.3)differences
  3.3)Combine Sparse Classes (<50 observations)
    3.3.1)group similar classes (ex.Wood Siding', 'Wood Shingle', and 'Wood' into 'Wood')
    3.3.2)group the remaining sparse classes into a single 'Other' class
  3.4)Add Dummy Variables (machine learning algorithms cannot handle text values)
    3.4.1)Remove Unused Features
  3.5)Remove Unused Features
    3.5.1)ID columns
    3.5.2)Features that wouldn't be available at the time of prediction
    3.5.3)Other text descriptions
  3.6)Encode Time
    3.6.1)https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/
    3.6.2)https://stats.stackexchange.com/questions/126230/optimal-construction-of-day-feature-in-neural-networks
    3.6.3)https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes
    
4) Feature Selection https://en.wikipedia.org/wiki/Feature_selection
  4.1)Correlation Feature Selection (features highly correlated with the classification, yet uncorrelated to each other)
  4.2)Guided Regularized Random Forest
  4.3)Metaheuristics Methods
    4.3.1)Fast Correlation Based Filter
    4.3.2)Wrapper methods evaluate Subsets of Variables
    4.3.3)Embedded methods combine the advantages of both previous methods
      
Welch's t-test  **Filter**	kernel support vector machine	Classification accuracy (10-fold)
Binary PSO + Mutation   **Wrapper**	Decision tree	weighted cost
Genetic Algorithm   **Wrapper**	Support Vector Machine	Sensitivity and specificity
Simulated Annealing   Naive bayesian	Classification accuracy (5-fold)
Symmetrical Tau (ST)  **Filter** 	Structural Associative Classification 	Accuracy, Coverage
Genetic Algorithm   **Wrapper**	Decision Tree	Classification accuracy (10-fold)
Infinite Feature Selection  **Filter** 	Independent 	Average Precision, ROC AUC
Eigenvector Centrality FS  **Filter** 	Independent 	Average Precision, Accuracy, ROC AUC
Feature Selection using Feature Similarity	**Filter**		r2
Genetic Algorithm	**Wrapper**	All paired Support Vector Machine	Classification accuracy (Leave-one-out cross-validation)
Hybrid Genetic Algorithm	**Wrapper**	K Nearest Neighbors	Classification accuracy (Leave-one-out cross-validation)
Genetic Algorithm		Support Vector Machine	Classification accuracy (10-fold)
Simulated Annealing	**Wrapper**	Regression	AIC, r2
HillClimbing	**Filter + Wrapper**	Naive Bayesian	Predicted residual sum of squares
Simulated Annealing, Genetic Algorithm	**Wrapper**	Regression	BIC
Genetic Algorithm	**Wrapper**	Support Vector Machine	EH-DIALL, CLUMP
Genetic Algorithm	**Wrapper**	K Nearest Neighbors	Classification accuracy (Leave-one-out cross-validation)
Genetic Algorithm	**Hybrid**	Support Vector Machine	Classification accuracy (Leave-one-out cross-validation)
Genetic Algorithm	**Embedded**	Support Vector Machine	Classification accuracy (10-fold)
Iterated Local Search	**Wrapper**	Regression	Posterior Probability
Genetic Algorithm + Iterated Local Search	**Embedded**	Support Vector Machine	Classification accuracy (10-fold)
Tabu Search + PSO	**Wrapper**	Support Vector Machine, K Nearest Neighbors	Euclidean Distance
Genetic Algorithm	**Wrapper**	Multiple Linear Regression, Partial Least Squares	root-mean-square error of prediction
PSO + Genetic Algorithm	**Wrapper**	Support Vector Machine	Classification accuracy (10-fold)
Ants colony	**Wrapper**	Artificial Neural Network	MSE

  4.4)Feature selection embedded in learning algorithms
    4.1)l1-regularization techniques, such as sparse regression, LASSO, and l1-SVM
    4.2)Regularized trees, e.g. regularized random forest implemented in the RRF package
    4.3)Decision tree
    4.4)Memetic algorithm
    4.5)Random multinomial logit (RMNL)
    4.6)Auto-encoding networks with a bottleneck-layer
    4.7)Submodular feature selection
    4.8)Local learning based feature selection
    4.9)Recommender system based on feature selection    
  4.5)Dimensionality Reduction https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/
    4.5.1)Factor based
      4.5.1.1)Factor analysis
      4.5.1.2)PCA
      4.5.1.3)ICA
    4.5.2)Projection based
      4.5.2.1)ISOMAP
      4.5.2.2)t-SNE
      4.5.2.3)UMAP

5) Regression (R http://topepo.github.io/caret/index.html)
  5.1)Identifying Seasonal Models and R Code (https://newonlinecourses.science.psu.edu/stat510/lesson/4/4.2)

6) Classification (R http://topepo.github.io/caret/index.html)
  6.1) Clustering 
  (http://www.differencebetween.net/technology/differences-between-supervised-learning-and-unsupervised-learning/)
  (http://share.opsy.st/56e7090e92b6c-MathWorks_Figure+1_Machine+Learning+Types.jpg)
    6.1.1) Unsupervised clustering
    6.1.2) Supervised clustering
      (https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/)
      (https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/)
      6.1.2.1)Connectivity 
      6.1.2.2)Centroid 
      6.1.2.3)Distribution 
      6.1.2.4)Density 
  6.2) MultiLabels
  6.3) Binary

7) Reinforcement (Python https://github.com/dennybritz/reinforcement-learning)
  
